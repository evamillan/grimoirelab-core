# -*- coding: utf-8 -*-
#
# Copyright (C) GrimoireLab Contributors
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <http://www.gnu.org/licenses/>.
#
# Authors:
#     Santiago Due√±as <sduenas@bitergia.com>
#     Jose Javier Merchante <jjmerchante@bitergia.com>
#

from __future__ import annotations

import copy
import datetime
import logging
import pickle

from typing import TYPE_CHECKING
from typing import Any

import django_rq

import perceval.backend
import perceval.backends

from rq.job import Job
from grimoirelab_toolkit.datetime import str_to_datetime

from .backends.utils import get_backend
from .common import Q_STORAGE_ITEMS, MAX_JOB_RETRIES
from .errors import NotFoundError
from .models import FetchTask

if TYPE_CHECKING:
    from logging import LogRecord
    from redis import Redis

logger = logging.getLogger(__name__)


class JobLogHandler(logging.StreamHandler):
    """Handler class for the job logs"""

    def __init__(self, job: Job) -> None:
        logging.StreamHandler.__init__(self)
        self.job = job
        self.job.meta['log'] = []
        self.job.save_meta()

    def emit(self, record: LogRecord) -> None:
        log = {
            'created': record.created,
            'msg': self.format(record),
            'module': record.module,
            'level': self.level,
        }
        self.job.meta['log'].append(log)
        self.job.save_meta()


class JobResult:
    """Class to store the result of a Perceval job.

    It stores the summary of a Perceval job and other useful data
    such as the task and job identifiers, the backend and the
    category of the items generated.

    :param job_id: job identifier
    :param task_id: task identifier linked to this job
    :param backend: backend used to fetch the items
    :param category: category of the fetched items
    """

    def __init__(self, job_id: str, task_id: str, backend: str, category: str):
        self.job_id = job_id
        self.task_id = task_id
        self.backend = backend
        self.category = category
        self.summary = None

    def to_dict(self) -> dict[str, str | int]:
        """Convert object to a dict"""

        result = {
            'job_id': self.job_id,
            'task_id': self.task_id
        }

        if self.summary:
            result['fetched'] = self.summary.fetched
            result['skipped'] = self.summary.skipped
            result['min_updated_on'] = self.summary.min_updated_on.timestamp()
            result['max_updated_on'] = self.summary.max_updated_on.timestamp()
            result['last_updated_on'] = self.summary.last_updated_on.timestamp()
            result['last_uuid'] = self.summary.last_uuid
            result['min_offset'] = self.summary.min_offset
            result['max_offset'] = self.summary.max_offset
            result['last_offset'] = self.summary.last_offset
            result['extras'] = self.summary.extras

        return result


class PercevalJob(Job):
    """Class for wrapping Perceval jobs.

    Wrapper for running and executing Perceval backends. The items
    generated by the execution of a backend will be stored on the
    Redis queue named `qitems`. The result of the job can be obtained
    accessing to the property `result` of this object.
    """

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self._perceval_result = None
        self._big = None

    @classmethod
    def create(cls, *args, **kwargs) -> PercevalJob:
        """Override the create method to run this Class"""

        if len(args) > 0:
            args = list(args)
            args[0] = cls.run
        else:
            kwargs['func'] = cls.run

        if 'on_success' not in kwargs or not kwargs['on_success']:
            kwargs['on_success'] = cls.on_success_callback
        if 'on_failure' not in kwargs or not kwargs['on_failure']:
            kwargs['on_failure'] = cls.on_failure_callback

        return super().create(*args, **kwargs)

    @classmethod
    def enqueue_job(cls, task: FetchTask, scheduled_datetime: datetime.datetime | None = None) -> PercevalJob:
        if not scheduled_datetime:
            scheduled_datetime = datetime.datetime.now(datetime.timezone.utc)

        job_args = cls._build_job_arguments(task)

        # TODO: should result_ttl be always set to -1?
        job = django_rq.get_queue(task.queue, job_class=PercevalJob).enqueue_at(
            datetime=scheduled_datetime,
            f=PercevalJob.run,
            result_ttl=-1,
            job_timeout=-1,
            **job_args,
        )

        task.status = FetchTask.Status.ENQUEUED
        task.age += 1
        task.scheduled_datetime = scheduled_datetime
        task.job_id = job.id
        task.save()

        logger.info(
            f"Job #{job.id} (task: {job_args['task_id']}) ({job_args['backend']})"
            f" enqueued in '{task.queue}' at {scheduled_datetime}"
        )

        return job

    @property
    def perceval_result(self) -> JobResult:
        has_big_summary = self._big and self._big.summary

        if not self._perceval_result.summary and has_big_summary:
            self._perceval_result.summary = self._big.summary

        return self._perceval_result

    @staticmethod
    def _build_job_arguments(task: FetchTask) -> dict[str, Any]:
        """Build the set of arguments required for running a job"""

        job_args = {}
        job_args['qitems'] = Q_STORAGE_ITEMS
        job_args['task_id'] = task.task_id

        # Backend parameters
        job_args['backend'] = task.backend
        backend_args = copy.deepcopy(task.backend_args)

        if 'next_from_date' in backend_args:
            backend_args['from_date'] = backend_args.pop('next_from_date')

        if 'next_offset' in backend_args:
            backend_args['offset'] = backend_args.pop('next_offset')

        if 'from_date' in backend_args and isinstance(backend_args['from_date'], str):
            backend_args['from_date'] = str_to_datetime(backend_args['from_date'])

        job_args['backend_args'] = backend_args

        # Category
        job_args['category'] = task.category

        return job_args

    def run(
        self,
        task_id: str,
        qitems: str,
        backend: str,
        category: str,
        backend_args: dict[str, Any],
    ) -> JobResult:
        """Run the backend with the given parameters.

        The method will run the backend assigned to this job,
        storing the fetched items in a Redis queue. The ongoing
        status of the job, can be accessed through the property
        `result`.

        Any exception during the execution of the process will
        be raised.

        :param task_id: ID of the task
        :param qitems: Redis queue to store items
        :param backend: Perceval backend to run
        :param category: Perceval backend category to run
        :param backend_args: parameters used to run the backend
        """

        try:
            bklass = perceval.backend.find_backends(perceval.backends)[0][backend]
        except KeyError:
            raise NotFoundError(element=backend)

        backend_args = backend_args.copy()

        self._perceval_result = JobResult(self.get_id(), task_id, backend, category)

        self._big = perceval.backend.BackendItemsGenerator(bklass,
                                                           backend_args,
                                                           category)

        for item in self._big.items:
            self._metadata(item)
            self.connection.rpush(qitems, pickle.dumps(item))

        return self._perceval_result

    def _metadata(self, item: dict[str, Any]) -> None:
        """Add metadata to an item.

        Method that adds in place metadata to Perceval items such as
        the identifier of the job that generated it or the version of
        the system.

        :param item: an item generated by Perceval
        """
        # TODO: add Grimoirelab core version?
        item['job_id'] = self.get_id()

    def _add_log_handler(self):
        """Add log handler to the job"""

        self.job_logger = JobLogHandler(self)
        for logger_name in [__name__, 'perceval', 'rq']:
            logger_job = logging.getLogger(logger_name)
            logger_job.setLevel(logging.INFO)
            logger_job.addHandler(self.job_logger)

    def _remove_log_handler(self):
        for logger_name in [__name__, 'perceval', 'rq']:
            logger_job = logging.getLogger(logger_name)
            logger_job.removeHandler(self.job_logger)

    def _execute(self):
        try:
            self._add_log_handler()
            return self.run(*self.args, **self.kwargs)
        finally:
            self.meta['result'] = self.result
            self.save_meta()
            self._remove_log_handler()

    @staticmethod
    def on_success_callback(job: Job, connection: Redis, result: Any, *args) -> None:
        """Reschedule the job based on the interval defined by the task.

        The new arguments for the job are obtained from the task
        object. This way if the object is updated between runs it
        will use the updated arguments.
        """
        try:
            task = FetchTask.objects.get(job_id=job.id)
        except FetchTask.DoesNotExist:
            logger.error("FetchTask not found. Not rescheduling.")
            return

        task.last_execution = datetime.datetime.now(datetime.timezone.utc)
        task.executions = task.executions + 1
        task.failed = False
        task.num_failures = 0

        backend = get_backend(task.backend)

        task.backend_args = backend.update_backend_args(result.summary, task.backend_args)

        scheduled_datetime = datetime.datetime.now(
            datetime.timezone.utc
        ) + datetime.timedelta(seconds=task.interval)
        task.status = FetchTask.Status.ENQUEUED
        job = PercevalJob.enqueue_job(task, scheduled_datetime=scheduled_datetime)
        task.job_id = job.id

        task.save()

    @staticmethod
    def on_failure_callback(job: Job, connection: Redis, t: Any, value: Any, traceback: Any):
        try:
            task = FetchTask.objects.get(job_id=job.id)
        except FetchTask.DoesNotExist:
            logger.error("FetchTask not found. Not rescheduling.")
            return

        task.last_execution = datetime.datetime.now(datetime.timezone.utc)
        task.num_failures += 1

        logger.error(f"Job #{job.id} (task: {task.id}) failed; error: {value}")

        task_max_retries = MAX_JOB_RETRIES

        try:
            bklass = perceval.backend.find_backends(perceval.backends)[0][task.backend]
        except KeyError:
            bklass = None

        if not bklass or not bklass.has_resuming():
            task.status = FetchTask.Status.FAILED
            logger.error(f"Job #{job.id} (task: {task.id}) unable to resume; cancelled")
        elif task.num_failures >= task_max_retries:
            task.status = FetchTask.Status.FAILED
            logger.error(f"Job #{job.id} (task: {task.id}) max retries reached; cancelled")
        else:
            logger.error(f"Job #{job.id} (task: {task.id}) failed but task will be resumed")

            task.status = FetchTask.Status.RECOVERY
            result = job.meta.get('result', None)

            if result:
                backend = get_backend(task.backend)
                task.backend_args = backend.recovery_params(
                    result.summary, task.backend_args
                )

            scheduled_datetime = datetime.datetime.now(
                datetime.timezone.utc
            ) + datetime.timedelta(seconds=60)  # TODO: error interval?
            job = PercevalJob.enqueue_job(task, scheduled_datetime=scheduled_datetime)
            task.job_id = job.id

        task.save()
